{"assets":{"B99053C8BBA9570FF5E201EA17D5277C":{"type":"texture","index":22,"assetRequest":{"type":"slide","state":"outgoing","slide":"F2C9B092-2E13-4A48-959D-F95D147E9F88"},"url":{"web":"assets/B99053C8BBA9570FF5E201EA17D5277C.png"},"width":1024,"height":768},"C3AEFC403EA57568B49A19F88C941549":{"type":"texture","index":23,"assetRequest":{"type":"slide","state":"incoming","slide":"965A75CF-D031-4CAF-8809-10B9188269DA"},"url":{"web":"assets/C3AEFC403EA57568B49A19F88C941549.svg"},"width":1024,"height":768}},"events":[{"effects":[{"beginTime":0,"baseLayer":{"animations":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":512,"pointY":384},"width":1024,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":251658240,"height":768,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"objectID":"0","layers":[{"animations":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":512,"pointY":384},"width":1024,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,-0.0004922987690640373,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":251658240,"height":768,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"layers":[{"animations":[],"layers":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":512,"pointY":384},"width":1024,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":0,"height":768,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"texture":"C3AEFC403EA57568B49A19F88C941549"},{"animations":[{"timeOffset":0,"from":{"scalar":false},"beginTime":0,"repeatCount":0,"fillMode":"both","property":"hidden","autoreverses":false,"duration":0.01,"to":{"scalar":true}}],"layers":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":512,"pointY":384},"width":1024,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":0,"height":768,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"texture":"B99053C8BBA9570FF5E201EA17D5277C"}]}]},"effects":[],"duration":0.01,"type":"transition","name":"none","objectID":"0"}],"automaticPlay":false,"hyperlinks":[],"accessibility":[{"text":"The raw frames preprocessed by first converting their RGB representation to grey-scale and down-sampling it to a 110×84 image. The final input representation obtained by cropping an 84 × 84 region of the image that roughly captures the playing area. The final cropping stage is only required because we use the GPU implementation of 2D convolutions from [11], which expects square inputs. For the experiments in this paper, the function φ from algorithm 1 applies this preprocessing to the last 4 frames of a history and stacks them to produce the input to the Q-function.","targetRectangle":{"y":214,"x":69.90000216166163,"width":889.3397460937499,"height":64}},{"text":"Used an architecture in which there is a separate output unit for each possible action, and only the state representation is an input to the neural network.","targetRectangle":{"y":278,"x":69.90000216166163,"width":847.5222656250003,"height":32.79999971389771}},{"text":"Since the scale of scores varies greatly from game to game, we fixed all positive rewards to be 1 and all negative rewards to be −1, leaving 0 rewards unchanged.  Clipping the rewards in this manner limits the scale of the error derivatives and makes it easier to use the same learning rate across multiple games. At the same time, it could affect the performance of our agent since it cannot differentiate between rewards of different magnitude.","targetRectangle":{"y":310.7999997138977,"x":69.90000216166163,"width":896.0211914062498,"height":64.79999971389771}},{"text":"Used the RMSProp algorithm with minibatches of size 32. The behavior policy during training was -greedy with  annealed linearly from 1 to 0.1 over the first million frames, and fixed at 0.1 thereafter.","targetRectangle":{"y":375.5999994277954,"x":69.90000216166163,"width":877.3057617187501,"height":48.79999971389771}},{"text":"Agent sees and selects actions on every kth frame (k = 4, usually) instead of every frame, and its last action is repeated on skipped frames.","targetRectangle":{"y":424.3999991416931,"x":69.90000216166163,"width":762.8106447586091,"height":32.79999981804286}},{"text":"Trained for a total of 10 / 50 million frames (that is, around 38 days of game experience in total) and used a replay memory of 1 million most recent frames.","targetRectangle":{"y":457.199998959736,"x":69.90000216166163,"width":862.3001953125,"height":32.79999971389771}},{"text":"We also found it helpful to clip the error term from the update e = yi -Q to be between -1 and 1. ( carpediem20의 코드는 이 부분이 구현이 안 되어 있음: 이 개발자 역시 함수를 불 연속적으로 만들었음.  역시 좀 이상함…)","targetRectangle":{"y":489.9999986736337,"x":69.90000216166163,"width":907.7067951376392,"height":49.82590303421028}},{"text":"∇L=𝔼[e∇Q(s,a;θ)] —> ∇L=𝔼[±1∇Q(s,a;θ)] (outside of [-1, 1])","targetRectangle":{"y":539.825901707844,"x":106.9000021616616,"width":334.80262265625,"height":35.79999971389771}},{"text":"L= 1/2𝔼[e2] ([−1,1])","targetRectangle":{"y":575.6259014217417,"x":143.9000021616616,"width":108.4997977814197,"height":35.79999971389771}},{"text":"L = 𝔼[|e|](outside of [−1,1])","targetRectangle":{"y":611.4259011356394,"x":143.9000021616616,"width":157.5729421875,"height":35.79999971389771}},{"text":"1 epoch = 50,000 minibatch weight updates = ~30 minutes of training time.  1 episode <= ~ 5 min","targetRectangle":{"y":647.2259008495371,"x":69.90000216166163,"width":538.7484374999998,"height":32.79999971389771}},{"text":"Model Specifics","targetRectangle":{"y":69,"x":304.625,"width":414.75,"height":85}}],"baseLayer":{"animations":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":512,"pointY":384},"width":1024,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":251658240,"height":768,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"objectID":"0","layers":[{"animations":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":512,"pointY":384},"width":1024,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,-0.0004922987690640373,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":251658240,"height":768,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"layers":[{"animations":[],"layers":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":512,"pointY":384},"width":1024,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":0,"height":768,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"texture":"B99053C8BBA9570FF5E201EA17D5277C"}]}]}}]}